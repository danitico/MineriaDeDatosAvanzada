---
title: "Tutorial on Imbalanced Classification Tasks"
author: "Daniel Ranchal Parrado (danielranchal@correo.ugr.es)"
date: "January 29, 2022"
always_allow_html: true
output:
  pdf_document: default
  html_notebook: default
---

This is an R Notebook a practical session on "Data Mining: Advanced
Aspects" course from the "Master in Data Science and Computer
Engineering" at University of Granada.

When you execute code within the notebook, the results appear beneath
the code.

# Introduction

Any dataset with an unequal class distribution is technically
imbalanced. However, a dataset is said to be imbalanced when there is a
significant, or in some cases extreme, disproportion among the number of
examples of each class of the problem. In other words, the class
imbalance occurs when the number of examples representing one class is
much lower than the ones of the other classes. Hence, one or more
classes may be underrepresented in the dataset. Such a simple definition
has brought along a lot of attention from researchers and practitioners
due to the number of real-world applications where the raw data gathered
fulfill this definition.

In this step-by-step tutorial you will:

1.  Get the most useful packages for addressing imbalanced
    classification with machine learning in R.
2.  Load a dataset and understand it's structure using statistical
    summaries and data visualization.
3.  Create several machine learning models in synergy with
    pre-processing techniques for imbalanced data, pick the best and
    build confidence that the predictive performance is reliable.

For the sake of easing this \`\`hands-on'' session, the document
contains chunks of R source code that can be run directly. Try executing
this first chunk by clicking the *Run* button within the chunk or by
placing your cursor inside it and pressing *Cmd+Shift+Enter*
(*Ctrl+Shift+Enter* in Windows).

```{r}
print("Welcome to your first R NoteBook")
```

If you like to a new chunk, just click the *Insert Chunk* button on the
toolbar or by pressing *Cmd+Option+I* (or *Ctrl+Alt+I* in Windows). This
way, you may add your own code if requested.

When you save the notebook, an HTML file containing the code and output
will be saved alongside it (click the *Preview* button or press
*Cmd+Shift+K* to preview the HTML file). This is actually a nice way to
compile all the tasks developed during the tutorial.

Here is an overview what we are going to cover in this *Tutorial on
Imbalanced Classification*:

1.  Installing the R platform.
2.  Loading the dataset.
3.  Summarizing the dataset.
4.  Visualizing the dataset.
5.  Evaluating some algorithms.
6.  Comparison among different solutions.

Take your time. Work through each step.

Install Required Packages Install the packages we are going to use
today. Packages are third party add-ons or libraries that we can use in
R.

```{r}
install.packages(c("caret","dplyr","pROC","tidyr","imbalance", "markdown", "mime", "ellipse", "themis"))
```

**NOTE:** We may need other packages, but caret should ask us if we want
to load them. If you are having problems with packages, you can install
the caret packages and all packages that you might need by typing
(remove the \# character):

```{r}
#install.packages("caret", dependencies=c("Depends", "Suggests"),repos = "http://cran.r-project.org")
```

Now, let's load the packages that we are going to use in this tutorial,
the caret and imbalance packages, among others.

```{r}
library(caret)
library(imbalance) #to be used in the optional part
library(dplyr)
library(pROC)
library(tidyr)
library(ellipse)
library(ggvis)
```

As you might already know, the caret package provides a consistent
interface into hundreds of machine learning algorithms and provides
useful convenience methods for data visualization, data resampling,
model tuning and model comparison, among other features. It's a must
have tool for machine learning projects in R.

One of its advantages for the use of caret is that it directly
integrates the use of pre-processing techniques when setting up the
control method for the training stage.

For more information about the caret R package see the [caret package
homepage](http://topepo.github.io/caret/index.html).

# Step One. Get your data

For the sake of establishing a controlled scenario, we will make use of
two different artificial data, namely the "circle" and "subclus" data,
which can be found in different studies on imbalanced classification.
These are binary problems, both in terms of attributes and classes.

In addition, we can load some data from the imbalance package, such as
the glass0 one (see [Kaggle](https://www.kaggle.com/uciml/glass)). In
this case, we can observe the behavior of the different methods on a
real case study.

Here is what we are going to do in this step:

0.  Load the glass0 data the easy way (with imbalance package). This
    will only serve for the individual tasks to be accomplished
    afterwards.
1.  Load the circle data from CSV (optional, for purists).
2.  Separate the data into a training dataset and a validation dataset.

For the "subclus" data, please follow the same procedure as in the case
of "circle".

## Load Data The Easy Way Fortunately

the imbalance package provides the
glass0 dataset for us, among others. Load the dataset as follows:

```{r}
# attach the glass0 dataset to the environment
data(glass0)
# rename the dataset
glassDataset <- glass0
```

You now have the glass0 loaded in R and accessible via the dataset
variable.

I like to name the loaded data "dataset". This is helpful if you want to
copy-paste code between projects and the dataset always has the same
name.

Load From CSV In the case you have previously downloaded the dataset,
you may want to load the data just from a CSV file.

1.  Download the circle (and subclus) dataset from PRADO (UGR).
2.  Save the file as circle.csv (and subclus) in your project directory.
3.  Load the dataset from the CSV file as follows:

```{r}
# load the CSV file from the local or web directory
dataset <- read.csv("data/circle.csv", header = TRUE)
# set the column names in the dataset (you must know them a priori)
colnames(dataset) <- c("Att1", "Att2", "Class")
dataset$Class <- relevel(factor(dataset$Class), "positive") #to ensure it appears at the first class
```

You now have the circle data loaded in R and accessible via the dataset
variable.

# Step Two. Know your data: Summarize Dataset
Now it is time to take a look at the data.

In this step we are going to take a look at the data a few different
ways:

1.  Dimensions of the dataset.
2.  Types of the attributes.
3.  Peek at the data itself.
4.  Levels of the class attribute.
5.  Breakdown of the instances in each class.
6.  Statistical summary of all attributes.

Don't worry, each look at the data is one command. These are useful
commands that you can use again and again on future projects.

## Dimensions of Dataset

We can get a quick idea of how many instances (rows) and how many
attributes (columns) the data contains with the dim function.

```{r}
# dimensions of dataset
dim(dataset)
```

You should see 2390 instances and 3 attributes in the case of the
"circle" data.

## Types of Attributes
It is a good idea to get an idea of the types of the attributes. They could be doubles, integers, strings, factors and other types.

Knowing the types is important as it will give you an idea of how to
better summarize the data you have and the types of transforms you might
need to use to prepare the data before you model it.

In this example, you should see that all of the inputs are double and
that the class value is a factor.

```{r}
# Check the structure and type of each attribute
str(dataset)
```

## Peek at the Data
It is also always a good idea to actually eyeball your data. You should see the first 5 rows of the data:

```{r}
# take a peek at the first 5 rows of the data
head(dataset)
```

## Levels of the Class
The class variable is a factor. A factor is a
class that has multiple class labels or levels. Let's look at the
levels. Notice how we can refer to an attribute by name as a property of
the dataset. In the results we can see that the class has 2 different
labels:

```{r}
# list the levels for the class
levels(dataset$Class)
```

This is a binary classification problem.

## Statistical Summary
Now finally, we can take a look at a summary of
each attribute.

This includes the mean, the min and max values as well as some
percentiles (25th, 50th or media and 75th e.g. values at this points if
we ordered all the values for an attribute). We can see here the uneven
distribution among classes: 2335 vs. 55 (circle data). This is confirmed
by computing the Imbalanced Ratio (IR).

```{r}
# summarize attribute distributions
summary(dataset)

imbalanceRatio(dataset)
```

# Visualize Dataset
We now have a basic idea about the data. We need to
extend that with some visualizations.

We are going to look at two types of plots:

1.  Univariate plots to better understand each attribute.
2.  Multivariate plots to better understand the relationships between
    attributes.

## Univariate Plots
We start with some univariate plots, that is, plots
of each individual variable.

It is helpful with visualization to have a way to refer to just the
input attributes and just the output attributes. Let's set that up and
call the inputs attributes x and the output attribute (or class) y.

```{r}
# split input and output
x <- dataset[,1:2]
y <- dataset[,3]
```

Given that the input variables are numeric, we can create box and
whisker plots of each. This gives us a much clearer idea of the
distribution of the input attributes:

```{r}
# boxplot for each attribute on one image
par(mfrow=c(1,2))
  for(i in 1:2) {
  boxplot(x[,i], main=names(dataset)[i])
}
```

We can also create a barplot (better a pie chart) of the class variable
to get a graphical representation of the class distribution. This
confirms what we learned in the last section, that the instances are
unevenly distributed across the two classes.

```{r}
# barplot for class breakdown
# plot(y)

# A pie chart is best
n_classes <- c(sum(y=="positive"), sum(y=="negative"))
pct <- round(n_classes/sum(n_classes)*100,digits=2)

lbls <- levels(dataset$Class)
lbls <- paste(lbls, pct) # add percents to labels
lbls <- paste(lbls,"%",sep="") # ad % to labels

pie(n_classes,labels = lbls, main="Class distribution")
```

## Multivariate Plots
Now we can look at the interactions between the
variables.

First let's look at scatterplots of all pairs of attributes and color
the points by class. In addition, because the scatterplots show that
points for each class are generally separate, we can draw ellipses
around them. We aim to see relationships between the input attributes
(trends) and between attributes and the class values (ellipses). In this
case the data is so simple that there is no clear conclusion.

```{r}
# scatterplot matrix
featurePlot(x=x, y=y, plot="ellipse")
```

We can also look at box and whisker plots of each input variable again,
but this time broken down into separate plots for each class. This can
help to tease out obvious linear separations between the classes.

```{r}
# box and whisker plots for each attribute
featurePlot(x=x, y=y, plot="box")
```

Scatter plots can give you a great idea of what you're dealing with: it
can be interesting to see how much one variable is affected by another.
In other words, you want to see if there is any correlation between two
variables. You can make scatterplots with the
[ggvis](http://www.rdocumentation.org/packages/ggvis) package, for
example.

```{r}
# Load in `ggvis`
library(ggvis)

# Dataset scatter plot
dataset %>% ggvis(~Att1, ~Att2, fill = ~Class) %>% layer_points()
```

# Evaluate Some Algorithms Now it is time to create some models of the
data and estimate their predictive ability on unseen data.

Here is what we are going to cover in this step:

1.  Set-up the test harness to use hold-out validation. It is not the
    best practice, but will serve us for teaching purposes.
2.  Apply different pre-processing approaches to the training data. Test
    must be kept unchanged.
3.  Build a kNN model to predict the class from each different training
    data.
4.  Compare and select the best methodology.

## Test Harness

As previously commented, we will focus on a hold-out partiton. This way,
we will have just one training and one test split, which may cause a
bias in our conclusions. However, it is shown how to proceed with a
proper cross validation procedure.

We must reset the random number seed before each run to ensure that the
evaluation of each algorithm is performed using exactly the same data
splits. It ensures the results are directly comparable.

```{r}
set.seed(42) #To ensure the same output

#An easy way to create split "data partitions":
trainIndex <- createDataPartition(dataset$Class, p = .75,list = FALSE, times = 1)

trainData <- dataset[ trainIndex,]
testData  <- dataset[-trainIndex,]

#Check IR to ensure a stratified partition
imbalanceRatio(trainData)
imbalanceRatio(testData)

#Ad hoc FCV
#testIndices <- createFolds(dataset$Class, k=5)
#First partition
#dataTrain <- dataset[-testIndices[[1]],]
#dataTest  <- dataset[testIndices[[1]],]
```

## Build Models
We don't know which algorithms would be good on this
problem or what configurations to use.

Let's evaluate 3 different methodologies with kNN:

1.  Original dataset (raw data)
2.  Trivial sampling techniques (random over and under sampling).
3.  SMOTE oversampling.

First of all, we need to create two auxiliar functions for the learning
and estimation stages. Please take into account that we are optimising
the k parameter of kNN via "grid search". If we remove this part, we may
also build a general function for any possible classification algorithm.

1)  Learning

```{r}
# a) Learning function
learn_model <-function(dataset, ctrl, message){
  model.fit <- train(
    Class ~ .,
    data = dataset,
    method = "knn",
    trControl = ctrl,
    preProcess = c("center","scale"),
    metric="ROC",
    tuneGrid = expand.grid(k = c(1,3,5,7,9,11))
  )

  model.pred <- predict(model.fit,newdata = dataset)

  #Get the confusion matrix to see accuracy value and other parameter values
  model.cm <- confusionMatrix(model.pred, dataset$Class,positive = "positive")
  model.probs <- predict(model.fit,newdata = dataset, type="prob")
  model.roc <- roc(dataset$Class,model.probs[,"positive"],color="green")

  return(model.fit)
}
```

2)  Prediction:

```{r}
# b) Estimation function
test_model <-function(dataset, model.fit, message){
  model.pred <- predict(model.fit, newdata = dataset)

  #Get the confusion matrix to see accuracy value and other parameter values
  model.cm <- confusionMatrix(model.pred, dataset$Class, positive = "positive")
  print(model.cm)

  model.probs <- predict(model.fit,newdata = dataset, type="prob")
  model.roc <- roc(dataset$Class,model.probs[,"positive"])

  plot(model.roc, type="S", print.thres= 0.5, main=c("ROC Test",message),col="blue")

  return(model.cm)
}
```

## Learn and store different models of the data

First, we check the obtained results from original data. Please recall
that an internal CV validation is used to set the best parameters for
the kNN model (see above). This is stated in the *trainControl* call.

```{r}
#Execute model ("raw" data)
ctrl <- trainControl(
  method="repeatedcv",
  number=5,
  repeats = 3,
  classProbs=TRUE,
  summaryFunction = twoClassSummary
)

model.raw <- learn_model(trainData, ctrl, "RAW")

#We may decide to plot the results from the grid search of the model's parameters
plot(model.raw, main="Grid Search RAW")
print(model.raw)

cm.raw <- test_model(testData,model.raw,"RAW ")
```

Now, we include the preprocessing stage into the control method of
*caret* and obtain new models. First with a simple undersampling
technique. The use of *RUS* should obtain a training set that is
perfectly balanced by removing instances from the majority class in a
random way.

```{r}
#Execute model ("preprocessed" data)
#Undersampling
ctrl <- trainControl(method="repeatedcv", number=5, repeats = 3, classProbs=TRUE, summaryFunction = twoClassSummary, sampling = "down")

model.us <- learn_model(trainData, ctrl, "US")
cm.us <- test_model(testData, model.us, "US")
```

Now we must check the behavior of the random oversampling approach. The
use of *ROS* should obtain a training set that is perfectly balanced by
replicating instances from the minority class in a random way.

```{r}
#Oversampling
ctrl <- trainControl(method="repeatedcv", number=5, repeats = 3,
                     classProbs=TRUE, summaryFunction = twoClassSummary, sampling = "up")
model.os <- learn_model(trainData,ctrl,"OS ")
cm.os <- test_model(testData,model.os,"OS ")
```

Finally we test the SMOTE state-of-the-art solution. Applying SMOTE
should obtain a perfectly balanced training set, creating new instances
from the minority class.

```{r}
#SMOTE
ctrl <- trainControl(method="repeatedcv", number=5, repeats = 3, classProbs=TRUE,summaryFunction = twoClassSummary, sampling = "smote")
model.smt <- learn_model(trainData,ctrl,"SMT ")
cm.smt <- test_model(testData,model.smt,"SMT ")
```

## Select Best Model
We now have 4 models learned from the same dataset
but with different pre-processing options. Each model comprises a
different accuracy estimation, and thus we need to compare the models to
each other and select the most "accurate" in terms of imbalanced
performance metrics, of course.

We can report on the performance of each model by first creating a list
of the created models and using the summary function:

```{r}
# summarize accuracy of models

models <- list(raw = model.raw,us = model.us,os = model.os,smt = model.smt)
results <- resamples(models)
summary(results)
```

We can also create a plot of the model evaluation results and compare
the spread and the mean performance of each model. In the case of circle
data, all preprocessing methods are equallly good in terms of
sensitivity (positive class recognition), but oversampling is a bit
better for specificity (negative class recognition).

The ideal procedure is to get a population of performance measures for
each algorithm because each algorithm when evaluating several times (k
fold cross validation).

```{r}
# compare accuracy of models
bwplot(results)
#dotplot(results)
```

We can finally make another different plot to compare additional metrics
for imbalanced classification, such as precision, recall and F1.

```{r}
#Carry out a comparison over all imbalanced metrics
comparison_metric <- function(models) {
  comparison <- data.frame(
    model = names(models),
    Sensitivity = rep(NA, length(models)),
    Specificity = rep(NA, length(models)),
    Precision = rep(NA, length(models)),
    F1 = rep(NA, length(models))
  )

  for (name in names(models)) {
    cm_model <- get(paste0("cm.", name))
    
    comparison[comparison$model == name, ] <- filter(comparison, model == name) %>% mutate(
      Sensitivity = cm_model$byClass["Sensitivity"],
      Specificity = cm_model$byClass["Specificity"],
      Precision = cm_model$byClass["Precision"],
      F1 = cm_model$byClass["F1"]
    )
  }

  comparison %>%
    gather(x, y, Sensitivity:F1) %>%
    ggplot(aes(x = x, y = y, color = model)) +
    geom_jitter(width = 0.2, alpha = 0.5, size = 3)
}

comparison_metric(models)
```

# Additional activities

The former tutorial served as a guide to understand the whole procedure
to be carried out when addressing a classification problem that presents
an uneven class distribution. Below, several tasks are proposed for
extending this Rmd file to check whether all concepts have been acquired
correctly.

## Activity 1: Extension with additional datasets. (mandatory)

Repeat the whole procedure carried out with "circle" data using now the
subclus problem. Please be sure to avoid unnecesary operations / R code
chunks.

```{r}
# load the CSV file from the local directory
subclus <- read.csv("data/subclus.csv", header=F)

# set the column names in the dataset
colnames(subclus) <- c("Attr1", "Attr2", "Class")
subclus$Class <- relevel(factor(subclus$Class), "positive")


# summarize statistical values
summary(subclus)
imbalanceRatio(subclus)

# Dataset scatter plot

subclus %>% ggvis(~Attr1, ~Attr2, fill = ~Class) %>% layer_points()

# Create split "data partitions":
set.seed(42) #To ensure the same output

#An easy way to create split "data partitions":
trainIndex <- createDataPartition(subclus$Class, p = .75,list = FALSE, times = 1)

trainData <- subclus[ trainIndex,]
testData  <- subclus[-trainIndex,]

#Check IR to ensure a stratified partition
imbalanceRatio(trainData)
imbalanceRatio(testData)

```

```{r}
model.raw <- learn_model(
  trainData,
  trainControl(
    method="repeatedcv",
    number=5,
    repeats = 3,
    classProbs=TRUE,
    summaryFunction = twoClassSummary,
  ),
  "raw"
)
cm.raw <- test_model(testData, model.raw, "raw")

model.undersampling <- learn_model(
  trainData,
  trainControl(
    method="repeatedcv",
    number=5,
    repeats = 3,
    classProbs=TRUE,
    summaryFunction = twoClassSummary,
    sampling = "down"
  ),
  "undersampling"
)
cm.undersampling <- test_model(testData, model.undersampling, "undersampling")

model.oversampling <- learn_model(
  trainData,
  trainControl(
    method="repeatedcv",
    number=5,
    repeats = 3,
    classProbs=TRUE,
    summaryFunction = twoClassSummary,
    sampling = "up"
  ),
  "oversampling"
)
cm.oversampling <- test_model(testData, model.oversampling, "oversampling")

model.smote <- learn_model(
  trainData,
  trainControl(
    method="repeatedcv",
    number=5,
    repeats = 3,
    classProbs=TRUE,
    summaryFunction = twoClassSummary,
    sampling = "smote"
  ),
  "smote"
)
cm.smote <- test_model(testData, model.smote, "smote")

# Summarize performance of models

models <- list(
  raw = model.raw,
  undersampling = model.undersampling,
  oversampling = model.oversampling,
  smote = model.smote
)
results <- resamples(models)
summary(results)

# Carry out a visual comparison over all imbalanced metrics
bwplot(results)
comparison_metric(models)
```

**_Si nos fijamos en los diagramas de cajas, que muestran las métricas ROC, Sensibilidad y Especificidad y cuyo conjunto de resultados se han obtenido con los diferentes folds y la técnicas de balanceo, se puede observar que el modelo básico "raw" tiene bastantes problemas en clasificar las instancias de la clase minoritaria (sensibilidad) mientras que clasifica bastante bien los ejemplos de la clase mayoritaria (especificidad)._**

**_Respecto a aplicar oversampling y undersampling aleatorio, se observa un trade-off bastante claro entre la sensibilidad y especificidad. Mientras que mejora de una manera considerable la sensibilidad y por lo tanto, la capacidad de clasificar correctamente las instancias de la clase minoritaria es mucho mejor, la especificidad empeora levemente si lo comparamos con la especificad del modelo base o raw._**

**_Por último, la aplicación de SMOTE hace que mejore la capacidad de clasificar las instancias de la clase minoritaria respecto al modelo original y además consigue que el trade-off entre sensibilidad y especificidad no sea perjudicial para este último. De hecho, consigue tener una especificidad muy similar al del modelo base. Lo único que habría que destacar sobre la aplicación de SMOTE es que la métrica sensibilidad presenta una distancia intercuartil bastante considerable, y por lo tanto una gran dispersión entre los resultados, cosa que no ocurre con undersampling y oversampling._**

**_Ahora nos fijamos en la gráfica en la que se muestran las métricas F1, Precisión, Sensibilidad y Especificidad. Se tiene que indicar que F1 es una métrica que se calcula utilizando la precisión y la sensibilidad (también llamada recall), y también que la precisión se calcula como el ratio entre aquellos que son verdaderamente de la clase positiva y la suma de los que ha considerado el modelo como de la clase positiva._**

**_En esta gráfica se puede observar que el modelo base (sin aplicarle ninguna técnica de balanceo) predomina con la mejor métrica en F1, precisión y especificidad. Como el objetivo de las técnicas de balanceo es hacer que el modelo clasifique mejor la clase minoritaria, y que por lo tanto, clasifique las dos clases de la manera correcta, habría que ver aquellos que tienen una métrica buena en sensiblidad y especificidad, y en el que el trade-off sea favorable para los dos. Como se ha comentado en la gráfica anterior, el método que conseguía esto era SMOTE._**

**_Aunque aplicando SMOTE solo se consigue que la métrica de sensibilidad sea solo la tercera mejor (casi un 0.85), consigue también mantener la métrica de la especificidad y que no se desplome, tal y como pasa con el undersampling y el oversampling. Por último, si nos fijamos en la métrica F1, vemos que el modelo en el que se aplica SMOTE y el modelo base rinden de manera similar mientras que en la precisión el modelo base es mucho mejor._**

**_En el caso en el que no nos importe clasificar algunas instancias negativas (las que son cercanas al "cluster" de instancias positivas o están dentro de ese cluster) y clasificar mejor las instancias de la clase minoritaria, el mejor modelo a considerar es aquel en el que se aplica SMOTE._**


## Activity 2: Using the "imbalance" library (mandatory)

As we have commented, there exists a CRAN package named as "imbalance"
that implements some of the most well-known data preprocesing techniques
for imbalanced classification. We must take a closer look to the
documentation in both the [imbalance package
homepage](https://github.com/ncordon/imbalance). or the help function
By using the "imbalance" library we may consider the application of
advanced techniques based on SMOTE. For that purpose, we must focus on
the "oversample" function.

It is your turn to select up to four different SMOTE techniques and
apply them over some of the datasets provided by the package (ecoli1,
glass0, haberman or yeast4, for example). Are there any significant
differences among the results?

```{r}
# Load the data
imbalanceRatio(glassDataset)

# Apply preprocessing with oversample function
glassDataset.smote <- imbalance::oversample(
  glassDataset,
  method="SMOTE",
  ratio = .8
)

glassDataset.mwmote <- imbalance::oversample(
  glassDataset,
  method="MWMOTE",
  ratio = .8
)

glassDataset.blsmote <- imbalance::oversample(
  glassDataset,
  method="BLSMOTE",
  ratio = .8
)

glassDataset.dbsmote <- imbalance::oversample(
  glassDataset,
  method="DBSMOTE",
  ratio = .8
)

# Check results with kNN, DT or any other classifier

trainControlGlass <- trainControl(
  classProbs = T,
  summaryFunction = twoClassSummary,
  method="repeatedcv",
  number=5,
  repeats = 3,
)

model.smote.glass <- train(
  Class ~ .,
  data = glassDataset.smote,
  method = "knn",
  trControl = trainControlGlass,
  preProcess = c("center","scale"),
  metric="ROC",
  tuneGrid = expand.grid(k = c(1,3,5,7,9,11))
)

model.mwmote.glass <- train(
  Class ~ .,
  data = glassDataset.mwmote,
  method = "knn",
  trControl = trainControlGlass,
  preProcess = c("center","scale"),
  metric="ROC",
  tuneGrid = expand.grid(k = c(1,3,5,7,9,11))
)

model.blsmote.glass <- train(
  Class ~ .,
  data = glassDataset.blsmote,
  method = "knn",
  trControl = trainControlGlass,
  preProcess = c("center","scale"),
  metric="ROC",
  tuneGrid = expand.grid(k = c(1,3,5,7,9,11))
)

model.dbsmote.glass <- train(
  Class ~ .,
  data = glassDataset.dbsmote,
  method = "knn",
  trControl = trainControlGlass,
  preProcess = c("center","scale"),
  metric="ROC",
  tuneGrid = expand.grid(k = c(1,3,5,7,9,11))
)

smote.results <- data.frame(
  smote = model.smote.glass$results$ROC,
  mwmote = model.mwmote.glass$results$ROC,
  blsmote = model.blsmote.glass$results$ROC,
  dbsmote = model.dbsmote.glass$results$ROC
)

friedman.result <- friedman.test(as.matrix(smote.results))
friedman.result
groups <- rep(1:dim(smote.results)[2], each=dim(smote.results)[1])
post.hoc <- pairwise.wilcox.test(
  as.matrix(smote.results),
  groups,
  p.adjust = "holm",
  paired = T
)
post.hoc
```

**_Se ha utilizado el dataset Glass0 y los métodos de smote base, mwsmote, blsmote y dbsmote. Para estos algoritmos hay que configurar que imbalance ratio se desea para el nuevo dataset. Teniendo en cuenta que el imbalance ratio del dataset original es del 0.4861111, se ha puesto como imbalance ratio deseado un 0.8. Para poder comprobar las técnicas de balanceo, se ha ejecutado un kNN para cada nuevo dataset y el original utilizando la función de caret. Para comprobar si hay diferencias significativas, se ejecuta el test de friedman y se obtiene un p-value de 0.01545, cuyo valor es menor que 0.05, por lo que se rechaza la hipótesis nula y se puede afirmar que existen diferencias significativas entre entre al menos un par de algoritmos._**

**_Para poder ver en que par de algoritmos existen diferencias significativas, se ejecuta a continuación un test post-hoc de holm. Con un 80% de confianza, se puede observar que los algoritmos 1 y 4, 2 y 4, y 3 y 4 se consideran equivalentes. Al igual pasa entre los algoritmos 2 y 3. Además se pueden apreciar diferencias significativas entre los algoritmos 2 y 1 a favor del algoritmo 2 (mwmote) y también entre los algoritmos 3 y 1 a favor del algoritmo 3 (blsmote)_**


Now, make a plot comparison between the original and preprocessed
dataset (only for SMOTE, for example). Recall that, being a 2D plot, you
must only two of the input columns. Alternatively, you can carry out a
"tsne" prior to the plot.

```{r}
# PCA
originaldataset.model.pca <- prcomp(~.-Class, data=glassDataset, rank. = 2)
smotedataset.model.pca <- prcomp(~.-Class, data=glassDataset.smote, rank. = 2)

original.pca.dataset <- data.frame(
  predict(originaldataset.model.pca, newdata = glassDataset)
)
original.pca.dataset$Class <- glassDataset$Class

smote.pca.dataset <- data.frame(
  predict(smotedataset.model.pca, newdata = glassDataset.smote)
)
smote.pca.dataset$Class <- glassDataset.smote$Class

# Visualize the data distribution between original and preprocess data.
# Original dataset
original.pca.dataset %>% ggvis(~PC1, ~PC2, fill = ~Class) %>% layer_points()

# Dataset after smote
smote.pca.dataset %>% ggvis(~PC1, ~PC2, fill = ~Class) %>% layer_points()
```

## Activity 3: Analyze the behavior of SMOTE preprocessing (optional)

In this last part of the practice, we intend to analyse the influence of
the different parameters of SMOTE. In fact, during the theoretical
classes, it was indicated that many of the SMOTE parameters could have a
certain importance in terms of the quality of the new synthetic examples
generated on the training set.

Therefore, the objective of this task is to contrast some of them to see
which ones can have more influence, or which ones can be significant
values to observe differences in the results.

To carry out this activity, the first issue is to determine the
experimental framework. As for the datasets to be used, "subclus" and
"circle" can be selected by default, although the study will be more
relevant when the number of problems, both synthetic and real, is
greater. In any case, the student should use a cross validation
technique to check the results.

As a base classifier to analyze the behavior, the student may use by
default kNN with K = 1 or K = 3. It could also be interesting to analyze
the results with the C4.5 decision tree or even Random Forest or any
other quality technique that the student consider to be appropriate.

Finally, it would remain to be discussed which parameters are
appropriate for the study, and what range of values to use. The most
direct parameters would be K for the number of neighbors chosen (for
example, between K = 1, K = 5, K = N/2 with N number of positive
instances, etc.), and the percentage of oversampling (double the
minority class, 50-50 class ratio, 50% minority over majority class,
among others).

To do so, you can either perform an ad hoc implementation of SMOTE, or
analyze among those available in the different R packages, the one that
allows you to perform a "Racing" of the parameters (clue: check the
current available imbalanced packages in CRAN).

Build the corresponding tables of results and make a brief analysis if
any interesting patterns are observed during the experimental analysis.

# Final Comments
Hope you enjoyed this tutorial about Imbalanced
Classification in Machine Learning. If you need further details on how
to perform any kind of task, please ask me via email at
[alberto\@decsai.ugr.es](mailto:alberto@decsai.ugr.es){.email}
